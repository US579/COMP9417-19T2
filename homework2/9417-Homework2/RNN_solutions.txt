--- setup for the current step
layer_input =  numpy.concatenate((outputs[step], X[:,step, :]), axis=1)
if step == self.input_size -1:
	weight = self.weights[1][:64,:]
else:
	weight = self.weights[0][:64,:]

--- calculate gradients
gradients, dW, db =  self.derivatives_of_hidden_layer(previous_gradients, layer_output, layer_input, weight)

--- update weights
self.weights[0] +=  self.learning_rate / X.shape[0] * dW
self.biases[0] += self.learning_rate/ X.shape[0] * db

--- setup for the next step 
previous_gradients = gradients
layer_output = outputs[step]
